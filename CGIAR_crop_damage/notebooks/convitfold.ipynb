{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7027067,"sourceType":"datasetVersion","datasetId":3963802}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from fastai.metrics import accuracy\nfrom fastai.optimizer import OptimWrapper\nfrom fastai.vision.all import *\nfrom PIL import Image\n\nfrom torch import optim\nimport torch.nn as nn\nimport timm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom transformers import AdamW\nimport cv2\n\nimport gc\nimport glob\nimport inspect\nimport pandas as pd\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\n\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage import color, transform\nfrom sklearn.model_selection import StratifiedKFold\n\nimport warnings \nwarnings.filterwarnings('ignore')\nset_seed(3, reproducible=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-10T10:03:55.608457Z","iopub.execute_input":"2024-01-10T10:03:55.609262Z","iopub.status.idle":"2024-01-10T10:04:05.673718Z","shell.execute_reply.started":"2024-01-10T10:03:55.609227Z","shell.execute_reply":"2024-01-10T10:04:05.672561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn_path = '/kaggle/input/zindidata/images/images/'\nfiles    = get_image_files(trn_path) ","metadata":{"execution":{"iopub.status.busy":"2024-01-10T10:04:22.834156Z","iopub.execute_input":"2024-01-10T10:04:22.835387Z","iopub.status.idle":"2024-01-10T10:04:52.012893Z","shell.execute_reply.started":"2024-01-10T10:04:22.835348Z","shell.execute_reply":"2024-01-10T10:04:52.012074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train            = pd.read_csv('/kaggle/input/zindidata/Train.csv')\ntest             = pd.read_csv('/kaggle/input/zindidata/Test.csv')\nSampleSubmission = pd.read_csv('/kaggle/input/zindidata/SampleSubmission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-10T10:04:57.245362Z","iopub.execute_input":"2024-01-10T10:04:57.246159Z","iopub.status.idle":"2024-01-10T10:04:57.353512Z","shell.execute_reply.started":"2024-01-10T10:04:57.246126Z","shell.execute_reply":"2024-01-10T10:04:57.352575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train = train.copy()\nTest = test.copy()\n\nskf = StratifiedKFold(5, shuffle=True, random_state=3)\nX = Train.drop(columns='damage')\ny = Train.damage\n\nfor fold, (_, valid_index) in enumerate(skf.split(X, y)):\n  Train.loc[valid_index, \"fold\"] = fold\n\nTrain.fold = Train.fold.astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"damage = Train.damage.unique()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T10:04:59.324199Z","iopub.execute_input":"2024-01-10T10:04:59.324815Z","iopub.status.idle":"2024-01-10T10:04:59.338016Z","shell.execute_reply.started":"2024-01-10T10:04:59.324786Z","shell.execute_reply":"2024-01-10T10:04:59.337028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights = torch.FloatTensor([1.3, 1.2, 1.4, 1.0, 2.8]).cuda()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T10:05:10.234761Z","iopub.execute_input":"2024-01-10T10:05:10.235557Z","iopub.status.idle":"2024-01-10T10:05:10.418281Z","shell.execute_reply.started":"2024-01-10T10:05:10.235524Z","shell.execute_reply":"2024-01-10T10:05:10.417362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_HEIGHT = IMG_WIDTH = 224\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\nINCEPTION_MEAN = INCEPTION_STD = (0.5, 0.5, 0.5)\n\n\nclass AlbumentationsTransform(RandTransform):\n    \"A transform handler for multiple `Albumentation` transforms\"\n    split_idx, order = None, 2\n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    def before_call(self, b, split_idx):\n        self.idx = split_idx\n    \n    def encodes(self, img: PILImage):\n        if self.idx == 0:\n            aug_img = self.train_aug(image=np.array(img))['image']\n        else:\n            aug_img = self.valid_aug(image=np.array(img))['image']\n        \n        return aug_img\n\n\ndef get_train_transforms(mean_std):\n    augmentations = [\n        A.VerticalFlip(p=.4),\n        A.HorizontalFlip(p=.5),\n        A.RandomRotate90(p=.2),\n        A.ShiftScaleRotate(\n            shift_limit=0.2, scale_limit=0.3, \n            rotate_limit=45, border_mode=0, p=.4\n        ),\n        A.RandomBrightnessContrast(brightness_limit=(-0.15,0.2), contrast_limit=(-0.1, 0.1), p=0.5),\n        A.Cutout(\n            max_h_size=int(IMG_HEIGHT*0.2),\n            max_w_size=int(IMG_WIDTH*0.2),\n            num_holes=4,\n            p=.7,\n        ),\n    ]\n    if mean_std=='imagenet':\n        augmentations.append(A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD))\n    elif mean_std=='inception':\n        augmentations.append(A.Normalize(mean=INCEPTION_MEAN, std=INCEPTION_STD))\n    else:\n        augmentations.append(A.Normalize(mean=0, std=1))\n\n    augmentations.append(ToTensorV2())\n    return A.Compose(augmentations)\n\n\ndef get_valid_transforms(mean_std):\n    augmentations = []\n    if mean_std=='imagenet':\n        augmentations.append(A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD))\n    elif mean_std=='inception':\n        augmentations.append(A.Normalize(mean=INCEPTION_MEAN, std=INCEPTION_STD))\n    else:\n        augmentations.append(A.Normalize(mean=0, std=1))\n\n    augmentations.append(ToTensorV2())\n    return A.Compose(augmentations)\n\ndef get_item_tfms(mean_std='imagenet'):\n    return [Resize(224, method='squish'), AlbumentationsTransform(get_train_transforms(mean_std), get_valid_transforms(mean_std))]","metadata":{"execution":{"iopub.status.busy":"2024-01-10T10:06:06.263126Z","iopub.execute_input":"2024-01-10T10:06:06.263515Z","iopub.status.idle":"2024-01-10T10:06:06.277946Z","shell.execute_reply.started":"2024-01-10T10:06:06.263484Z","shell.execute_reply":"2024-01-10T10:06:06.277051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(arch, item, batch, epoch=7):\n    m_name = 'V'\n    dblock = DataBlock(blocks = (ImageBlock, CategoryBlock(vocab=damage)),\n                       get_x = ColReader('filename', pref = trn_path),\n                       get_y = ColReader('damage'),\n                       splitter = RandomSplitter(valid_pct = 0.2, seed = 3),\n                       item_tfms = item, #Resize(320, method = 'squish'),\n                       batch_tfms = batch, #aug_transforms(size = 128, min_scale = 0.75)\n                      )\n\n    dls = dblock.dataloaders(Train, bs=98)\n    learn = vision_learner(dls, arch, loss_func = LabelSmoothingCrossEntropyFlat(weight=class_weights),\n                           metrics=[accuracy, error_rate, Precision(average='macro'), Recall(average='macro'), RocAuc(average='macro')]).to_fp16()\n    learn.fine_tune(epoch, 0.018)\n    dls.rng.seed(3)\n    interp = ClassificationInterpretation.from_learner(learn)\n    losses,idxs = interp.top_losses()\n    len(dls.valid_ds)==len(losses)==len(idxs)\n    interp.plot_confusion_matrix(figsize=(7,7))\n\n    return learn","metadata":{"execution":{"iopub.status.busy":"2024-01-10T10:06:19.628433Z","iopub.execute_input":"2024-01-10T10:06:19.629330Z","iopub.status.idle":"2024-01-10T10:06:19.636780Z","shell.execute_reply.started":"2024-01-10T10:06:19.629298Z","shell.execute_reply":"2024-01-10T10:06:19.635863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dls(fold=0, mean_std='imagenet'):\n    clas_block = DataBlock(blocks = (ImageBlock, CategoryBlock(vocab=damage)),\n                       get_x = ColReader('filename', pref = trn_path),\n                       get_y = ColReader('damage'),\n                       splitter = splitter=MaskSplitter(Train.fold == fold),\n                       item_tfms = get_item_tfms(mean_std), #Resize(320, method = 'squish'),\n                       batch_tfms = aug_transforms(min_scale= 0.75), #aug_transforms(size = 128, min_scale = 0.75)\n                      )\n    dls = dblock.dataloaders(Train, bs=32)\n    dls.rng.seed(21)\n\n    return dls","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def free_memory(to_delete: list):\n    calling_namespace = inspect.currentframe().f_back\n\n    for _var in to_delete:\n        calling_namespace.f_locals.pop(_var, None)\n        gc.collect()\n        torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T10:06:23.167323Z","iopub.execute_input":"2024-01-10T10:06:23.168169Z","iopub.status.idle":"2024-01-10T10:06:23.172933Z","shell.execute_reply.started":"2024-01-10T10:06:23.168131Z","shell.execute_reply":"2024-01-10T10:06:23.171861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor fold in range(5):\n    print('*'*25+f\"Fold {fold}\"+'*'*25)\n    m_name = f'fold-{fold}'\n    dls = get_dls(fold, 'imagenet')\n    learn = vision_learner(dls, 'convit_small.fb_in1k', \n                           loss_func = CrossEntropyLossFlat(weight=class_weights),\n                           metrics=[accuracy, error_rate, Precision(average='macro'), Recall(average='macro'), RocAuc(average='macro')])\n    \n    learn.fine_tune(5, 0.075)\n    \n    interp = ClassificationInterpretation.from_learner(learn)\n    losses,idxs = interp.top_losses()\n    len(dls.valid_ds)==len(losses)==len(idxs)\n    interp.plot_confusion_matrix(figsize=(7,7))\n    \n    valid = learn.dls.valid\n    preds1 , targets = learn.tta(dl=valid, n=4)\n    print(accuracy(preds1 , targets))\n    \n    test_dl = learn.dls.test_dl('/kaggle/input/zindidata/images/images/' + Test['filename'])\n    preds1 , _ = learn.tta(dl=test_dl)\n    preds1 = F.softmax(preds1, dim=1)\n    predictions.append(preds1)\n\n    free_memory([model, learn, preds1, dls])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    pred1 += predictions[i]\npred1 /= 5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_sf = pd.DataFrame({\n    \n    'ID': Test['ID'],\n    'DR': preds1[:, 0].squeeze().numpy(),\n    'G' : preds1[:, 1].squeeze().numpy(),\n    'ND': preds1[:, 2].squeeze().numpy(),\n    'WD': preds1[:, 3].squeeze().numpy(),\n    'other': preds1[:, 4].squeeze().numpy(),\n})\nsubmission_sf.to_csv('convitfold.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T14:36:42.627892Z","iopub.status.idle":"2024-01-09T14:36:42.628261Z","shell.execute_reply.started":"2024-01-09T14:36:42.628089Z","shell.execute_reply":"2024-01-09T14:36:42.628106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_sf","metadata":{"execution":{"iopub.status.busy":"2024-01-09T14:36:42.629419Z","iopub.status.idle":"2024-01-09T14:36:42.629728Z","shell.execute_reply.started":"2024-01-09T14:36:42.629574Z","shell.execute_reply":"2024-01-09T14:36:42.629589Z"},"trusted":true},"execution_count":null,"outputs":[]}]}